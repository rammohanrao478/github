---->kubernetes responsibilities include container deployment,scaling and descaling ,
    container load balancing
--->actually kubernetes is not a replacement of docker,but kubernees can be considered
    as replacement of dacker swarm,kubernetes significantly more complex than swarm
	and requiresmore work to deploy
	
kubernetes features
====================
1. auto sheduling
2.self healing capabilities:resceduling,replacing,restarting the containers which are died
3.automated rollout and roll back
4.horizental scaling and load balancing

API server
==========
kube API server interacts with API,it is a front end of the kubernetes control plane.communication
center for developers,system admin and other kubernetes compoents
API--Application progrrammable interface

scheduler
=========
scheduler wathes the pods and assigns the pods to run on specific hosts

kubernetes installation process:
================================
1.we have to take 3 instances.1 for master ,2 for worker nodes
2. sudo su in 3 instances
3.yum install docker -y
4.service docker start
5.swapoff -a
6.vi /etc/docker/daemon.json
{
 "exec-opts":  [native.cgroupdriver=systemd"]
}


7. sudo systemctl restart docker
8.CREATE A PROPER YUM REPO FILES
cat <<EOF > /etc/yum.repos.d/kubernetes.repos
[kubernetes]
name=kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-e17-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=0
gpgkey=https://packages.cloud.google.co/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/yum-key
exclude=kube*
EOF
9.
cat <<EOF > /etc/sysctl.dk8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
setenforce 0
10.yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
11.systemctl enable kubelet && systenctl start kubelet

the folloeing commands are executed on master only
12.kubeadm init
13.vi .bash_profile
export KUBECONFIG=/etc/kubernetes/admin.conf 


copy the long line at worker nodes to attach wirh master
kubeadm join

14.kubectl get nodes
now the nodes are not in ready state
now we have to enter the following command in master

15.kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
16.kubectl get nodes
now the nodes are in ready state
17.kubectl get all

kubernetes class2
==================
1.kubectl get pods
2.pod scipt:
vi pods.yml
apiVersion: v1
kind :Pod
metadata: 
   name: nodeapp
   lables:
      app: nodeapp
	  spec:
	     containers:
		 - name: nodeapp
		 image: kammana/nodeapp:v1
		 ports:
		   - container port: 8080


3.kubectl create -f pods.yml   ----this commandis used fir creation of pod by using the above file
4.kubectl get pods ---no of pods are available are are displayed here
5.kubectl get pods -o wide ---it gives complete information regarding pod
6.kubectl describe pod nodeapp ---it gives the complete information
7.kubectl exec -it nodeapp bash ---it is used to login to the pod
8.kubectl exec nodeapp printenv --we can print all details 
9.kubectl logs nodeapp ---we can see the what are the operations are performed in the pod can be obtained here
8.kubectl expose pods/nodeapp --type="NodePort" --port 8080 ---it can create service
9.kubectl get svc ---- this can be used for no of services in pod
10.kubectl get svc -o wide ---we can get complete information 
11.kubectl describe svc nodeapp
12.kubectl delete pod nodeapp ---this can delete the nodeapp pods/nodeapp
13.kubectl delete svc nodeapp --- this can delete the service 
14 kubectl create -f pods.yml
15.kubectl get pods

script for service
==================
16.vi svc.yml
kind service
apiVersion: v1
metadata:
 name: node app
 spec:selector
   app: nodeapp
   ports:
   protocal: TCP
   port :80
   targetport: 8080
   nodePort: 31000
   type: NodePort
   
17.kubectl create -f svc.yml  --- it can create the service from the above script
18.kubectl get svc
19.kubectl get pod  -o wide --- it can display the service where it is run
19.vi pods.yml

apiVersion: v1
kind :Pod
metadata: 
   name: nodeapp2
   lables:
      app: nodeapp
	  spec:
	     containers:
		 - name: nodeapp
		 image: kammana/nodeapp:v1
		 ports:
		   - container port: 8080
		 
here we modify only name:nodeapp2 

20.kubectl apply -f pods.yml
 here nodeapp2 is created

create :here this one is initially after writing the script of pod it can crete the pod
apply:if we can change or modify any in script we can use apply

21.kubectl get nodes 
noad app2 is also created here
22.kubectl get pods -o wide---here we can find the  where the pod is running
23.kubectl describe svc nodeapp 
24.  vi pods.yml

apiVersion: v1
kind :Pod
metadata: 
   name: nodeapp3
   lables:
      app: nodeapp
	  spec:
	     containers:
		 - name: nodeapp
		 image: kammana/nodeapp:v1
		 ports:
		   - container port: 8080
		   
25.kubectl apply -f pods.yml
26.kubectl get pods -----here the nodeapp3 is displayed
27.kubectl describe svc nodeapp --- here we can see that noad app 3 is also added .we can observe it through ipadresses


now i want to change the service 
vi svc.yml
vi svc.yml
kind service
apiVersion: v1
metadata:
 name: hariapp
 spec:
 selector:
   app: hariapp
   ports:
   protocal: TCP
   port :80
   targetport: 8080
   nodePort: 31005
   type: NodePort
   
28.kubectl get svc ---here we get the another service 

29 kubectl get pods 
30.cubectl delete pod nodeapp nodeapp1
31.kubectl get svc
32.kubectl delete svc nodeapp hariapp
33.kubectl create -f nodeapp.yml
34 kubectl get pods
vi svc.yml
kind service
apiVersion: v1
metadata:
 name: hariapp
 spec:
 selector:
   app: hariapp
   ports:
   protocal: TCP
   port :80
   targetport: 8080
   nodePort: 31005
   type: LoadBalancer

35 kubectl apply  -f svc.yml
36 kubectl get pods -o wide
now we have to pubicip of workernoe2 along eith port number 31005

now we have to go to replicas 
=============================
vi rs.yml
 
apiVersion : aps/v1
kind:ReplicaSet
metadeta:
  name:nodeapp
  spec:
replicas: 3
  selector:
  matchExpresions:  
      - {key: app,operator: In, values: [nodeapp]}
template:
 metadata:
 name: nodeapp
 labels: 
  app: nodeapp
  spec:
  containers:
   - name: nodeapp
   image: kammana/nodeapp:v1
   ports
   containerPort: 8080
   
   
   
37.kubectl create -f rs.yml
38 kubectl get pods ------- now we observe that 3 pods ,if we delete one of them then it is recreated because of thr replica script
39.kubectl delete pode pode name
40 kubectl get pods ---herewe can observe that pode is again created
here we to maintain change 3 pods to 7 pods
41.vi rs.yml
apiVersion : aps/v1
kind:ReplicaSet
metadeta:
  name:nodeapp
  spec:
replicas: 7
  selector:
  matchExpresions:  
      - {key: app,operator: In, values: [nodeapp]}
template:
 metadata:
 name: nodeapp
 labels: 
  app: nodeapp
  spec:
  containers:
   - name: nodeapp
   image: kammana/nodeapp:v1
   ports
   containerPort: 8080
 42.qubectl apply -f rs.yml
 43.kubectl get pods
 44.kubectl delete rs nodeapp
 
 45.vi deploy.yml
 apiVersion : aps/v1
kind:Deployment
metadeta:
  name:nodeapp
  spec:
replicas: 
stratagy:
    type:RollingUpdate
rollingUpdate:
maxSurge:25%
maxUnavailable:25%	
 selector:
  matchExpresions:  
      - {key: app,operator: In, values: [nodeapp]}
template:
 metadata:
 name: nodeapp
 labels: 
  app: nodeapp
  spec:
  containers:
   - name: nodeapp
   image: kammana/nodeapp: v1
   ports
   containerPort: 8080 

 46 kubectl create -f deploy.yml
   kubectl get deployments
47 vi svc.yml
kind: service
apiVersion:v1
metadata :
 name:nodeapp   
  spec:
  slector:
  app:nodeapp
  ports:
 - protocal: TCP
 port: 80
  targetport:8080
  nodePort:31006
  type: LoadBlaencer
  
 48.kubectl apply -f svc.yml
   
 49 vi deploy.yml
 
 apiVersion : aps/v1
kind:Deployment
metadeta:
  name:nodeapp
  spec:
replicas: 
stratagy:
    type:RollingUpdate
rollingUpdate:
maxSurge:25%
maxUnavailable:25%	
 selector:
  matchExpresions:  
      - {key: app,operator: In, values: [nodeapp]}
template:
 metadata:
 name: nodeapp
 labels: 
  app: nodeapp
  spec:
  containers:
   - name: nodeapp
   image: kammana/nodeapp: v2
   ports
   containerPort: 8080 
50.kubectl apply -f deploy.yml ---- due to this the deployment colour is change
51.kubectl apply -f svc.yml
 v1=====gives black colour
 v2======gives blue colour
 v3======gives red colour
 kubernetes can provide historycommands also .using this history we can roll back
52.kubectl rollout historydeployment nodeapp
53.kubectl rollout undo deploymentnodeapp --to-revision=1
if you want to rollback once again we have to take history once again
 54.kubectl rollout history deployment nodeapp
 55.kubectl rollout undo deploymentnodeapp --to-revision=2
 
 kubernetes project:
 ===================
 1.we  requre jenkins instance with maven git are installed in that instane.
 2.we require one k8s master and two worker nodes
 3.go to manage jenkins-----goto maage liugins---select k8s continuous deploy  plugin it is not working so we have to download the plugin though 
 following link
   https://updates.jenkins.io/download/plugins/kubernetes-cd/
   goto above link and download 1.0.0

goto mange plugins click on advanced
 select upload plugin
 install it
 
 now we have to create a jenkins job
 name k8 pipeline 
 job type pipeline
 